# Extreme Parkour 视觉系统详解

## 📸 概述

Extreme Parkour使用**深度相机**作为视觉传感器，在第二阶段训练中通过**视觉蒸馏**技术，让机器人学会用深度图像替代训练时使用的特权信息（如地形高度图、摩擦系数等）。

---

## 🎯 为什么使用深度相机？

### 对比：RGB相机 vs 深度相机

| 特性 | RGB相机 | 深度相机 |
|------|---------|----------|
| 输出 | 彩色图像 (3通道) | 距离图像 (1通道) |
| 信息量 | 纹理、颜色、光照 | **几何、距离** |
| 对光照敏感度 | 高 | 低 |
| 地形感知 | 需要学习 | **直接测量** |
| 网络复杂度 | 高（需要特征提取） | 低 |
| 训练时间 | 长 | 短 |

✅ **结论**：深度相机更适合机器人导航，因为：
- 直接提供障碍物距离信息
- 不受光照变化影响
- 网络结构更简单，推理更快

---

## 🔧 深度相机配置

### 1. 硬件参数

```python
class depth:
    # 相机安装位置（相对于机器人躯干中心）
    position = [0.27, 0, 0.03]  # [前方27cm, 水平居中, 高3cm]
    
    # 相机朝向
    angle = [-5, 5]             # [俯仰角-5°, 横滚角5°]
                                # 负值=向下看（看到脚前方地面）
    
    # 视场参数
    horizontal_fov = 87         # 水平视场角87度（较宽，能看到侧面）
    
    # 图像分辨率
    original = (106, 60)        # Isaac Gym输出: 宽106 x 高60像素
    resized = (87, 58)          # 网络输入: 宽87 x 高58像素
```

**可视化相机视野：**
```
                机器人俯视图
                    ↑前
              +---[相机]---+
              |     ▲      |
              |    /|\     |  ← 躯干
    左 ←------|   / | \    |-----→ 右
              |  /  |  \   |
              | ←-87°-→|   |
              +------------|
                   ↓后
                   
相机能看到：
- 前方2米内的地面
- 左右各约1.5米范围
- 能够提前看到台阶、障碍物
```

### 2. 深度测量范围

```python
near_clip = 0.0  # 最近可见距离：0米（从相机镜头开始）
far_clip = 2.0   # 最远可见距离：2米

# 深度值含义：
# -2.0m = 最远处（墙、远处障碍）
# -1.0m = 中等距离（前方地面）
# -0.0m = 最近处（机器人身体、近处障碍）
```

**深度图示例：**
```
地面视图（从相机看）：

█████████████  ← 近处台阶 (0.3m)
░░░░░░░░░░░░░  ← 平坦地面 (1.0m)
▓▓▓▓▓▓▓▓▓▓▓▓▓  ← 远处斜坡 (1.8m)
▒▒▒▒▒▒▒▒▒▒▒▒▒  ← 超远处 (>2.0m, 截断)

深色 = 近  灰色 = 中  浅色 = 远
```

---

## 📊 深度图像处理管道

### 完整处理流程

```python
# ========== 步骤1: 原始图像获取 ==========
# Isaac Gym输出: [106, 60] 像素, 深度值为负数
depth_raw = gym.get_camera_image_gpu_tensor(...)
# 示例值: depth_raw[30, 50] = -1.5 (距离1.5米)

# ========== 步骤2: 裁剪边缘 ==========
# 去除相机畸变严重的边缘区域
depth_cropped = depth_raw[:-2, 4:-4]
# [106, 60] → [106-4-4, 60-2] = [98, 58]

# ========== 步骤3: 添加噪声 ==========
# 模拟真实深度相机的测量误差
noise = uniform(-dis_noise, +dis_noise)  # dis_noise=0.0默认不加
depth_noisy = depth_cropped + noise

# ========== 步骤4: 限制范围 ==========
# 只保留0-2米范围内的深度
depth_clipped = clip(depth_noisy, -far_clip, -near_clip)
# 超出范围的值被截断到边界值

# ========== 步骤5: 调整分辨率 ==========
# 使用双三次插值缩放到网络输入尺寸
depth_resized = resize(depth_clipped, size=(58, 87), mode='bicubic')
# [98, 58] → [87, 58] (稍微缩小宽度)

# ========== 步骤6: 归一化 ==========
# 转换到 [-0.5, +0.5] 范围，方便神经网络处理
depth_normalized = (depth_resized * -1 - near_clip) / (far_clip - near_clip) - 0.5
#                   ↑翻转正负    ↑移到0-2        ↑缩放到0-1       ↑移到-0.5~0.5

# 最终输出: [58, 87] 的归一化深度图
```

### 归一化细节

```python
# 假设原始深度: -1.2m (1.2米距离)

步骤1: 翻转符号
-1.2 * -1 = 1.2

步骤2: 减去near_clip (平移到从0开始)
1.2 - 0.0 = 1.2

步骤3: 除以范围 (缩放到0-1)
1.2 / (2.0 - 0.0) = 0.6

步骤4: 减去0.5 (移到-0.5~0.5)
0.6 - 0.5 = 0.1

# 结果: 1.2米的物体 → 归一化值 0.1
```

**归一化映射表：**
| 真实距离 | 原始值 | 归一化值 | 语义 |
|---------|--------|----------|------|
| 0.0m | 0.0 | -0.5 | 紧贴相机 |
| 0.5m | -0.5 | -0.25 | 很近 |
| 1.0m | -1.0 | 0.0 | 中等距离 |
| 1.5m | -1.5 | 0.25 | 较远 |
| 2.0m | -2.0 | 0.5 | 最远 |

---

## 🧠 深度编码器网络

### 网络架构

```python
DepthOnlyFCBackbone58x87:

输入: [batch, 1, 58, 87]  # 单通道深度图
  ↓
┌─────────────────────────────────────┐
│ Conv2D(32 filters, 5x5 kernel)      │  提取局部几何特征
│ Output: [batch, 32, 54, 83]         │  (边缘、角点、表面)
└─────────────────────────────────────┘
  ↓
┌─────────────────────────────────────┐
│ MaxPool2D(2x2)                       │  降采样，增大感受野
│ Output: [batch, 32, 27, 41]         │
└─────────────────────────────────────┘
  ↓ ELU激活
┌─────────────────────────────────────┐
│ Conv2D(64 filters, 3x3 kernel)      │  提取中级特征
│ Output: [batch, 64, 25, 39]         │  (障碍物形状、地形走向)
└─────────────────────────────────────┘
  ↓ ELU激活
┌─────────────────────────────────────┐
│ Flatten                              │
│ Output: [batch, 62400]               │  展平为向量
└─────────────────────────────────────┘
  ↓
┌─────────────────────────────────────┐
│ Linear(62400 → 128)                  │  压缩特征
│ Output: [batch, 128]                 │
└─────────────────────────────────────┘
  ↓ ELU激活
┌─────────────────────────────────────┐
│ Linear(128 → 32)                     │  深度特征编码
│ Output: [batch, 32]                  │
└─────────────────────────────────────┘
  ↓
  Concat(depth_features + proprioception)
  ↓
┌─────────────────────────────────────┐
│ MLP(32+n_proprio → 128 → 32)        │  融合深度和本体感知
│ Output: [batch, 32]                  │  ← 估计的特权信息
└─────────────────────────────────────┘
```

### 网络参数量

```python
参数统计:
- Conv2D层: (5*5*1 + 1) * 32 = 832
- Conv2D层: (3*3*32 + 1) * 64 = 18,496
- Linear层: 62400 * 128 = 7,987,200
- Linear层: 128 * 32 = 4,096
- 融合MLP: (32+53) * 128 + 128*32 = 14,976

总参数: ~8M (8百万参数)
推理时间: ~2ms (RTX 3090)
显存占用: ~30MB
```

---

## 🎓 视觉蒸馏训练

### 两阶段训练对比

#### 阶段一：特权学习（Teacher）

```
                  ┌──────────────┐
                  │ 本体感知(53)  │
                  │ - 速度、姿态  │
                  │ - 关节状态等  │
                  └──────┬───────┘
                         │
    ┌────────────────────┼────────────────────┐
    │                    │                    │
┌───▼──────┐      ┌──────▼──────┐      ┌─────▼──────┐
│ 扫描点   │      │ 特权信息    │      │ 历史状态   │
│ (132)    │      │ (priv)      │      │ (530)      │
│          │      │ - 地形高度  │      │            │
│          │      │ - 摩擦系数  │      │            │
│          │      │ - 载荷等    │      │            │
└──────────┘      └─────────────┘      └────────────┘
    │                    │                    │
    └────────────────────┼────────────────────┘
                         │
                    ┌────▼─────┐
                    │ 策略网络 │
                    └────┬─────┘
                         │
                    ┌────▼─────┐
                    │ 12维动作 │
                    └──────────┘

训练15000次迭代，学会使用特权信息在复杂地形运动
```

#### 阶段二：视觉蒸馏（Student）

```
                  ┌──────────────┐
                  │ 本体感知(53)  │
                  └──────┬───────┘
                         │
    ┌────────────────────┼────────────────────┐
    │                    │                    │
┌───▼──────┐      ┌──────▼──────┐      ┌─────▼──────┐
│ 深度图像 │      │ 历史状态    │      │ 扫描点     │
│ (58x87)  │      │ (530)       │      │ (132)      │
└────┬─────┘      └─────────────┘      └────────────┘
     │                    │                    │
┌────▼─────────┐          │                    │
│ 深度编码器   │          │                    │
│ → 32维估计   │          │                    │
└────┬─────────┘          │                    │
     │                    │                    │
     └────────────────────┼────────────────────┘
                          │
                     ┌────▼─────┐
                     │ 策略网络 │  (冻结，不更新)
                     └────┬─────┘
                          │
                     ┌────▼─────┐
                     │ 12维动作 │
                     └──────────┘

训练10000次迭代，让深度编码器学会估计特权信息
```

### 蒸馏损失函数

```python
# 主要损失：深度编码器蒸馏损失
depth_encoder_loss = MSE(
    estimated_priv_info,    # 深度编码器的输出 [batch, 32]
    true_priv_info          # 阶段一使用的真实特权信息 [batch, 32]
)

# 辅助损失：策略性能保持
policy_loss = PPO_loss(...)  # 确保性能不下降

# 总损失
total_loss = policy_loss + λ * depth_encoder_loss
#            ↑继续优化策略  ↑蒸馏权重(如λ=1.0)

# 优化器只更新深度编码器参数
optimizer.step()  # 只更新depth_encoder的权重
```

### 训练曲线示例

```
深度编码器损失（MSE）：

Loss
 1.0 |██
     |██\
 0.8 |   ██
     |     ███
 0.6 |        ███
     |           ████
 0.4 |               ████
     |                   █████
 0.2 |                        ██████████
 0.0 |_____________________________________
     0    2k    4k    6k    8k    10k
                 Iteration

平均奖励（保持稳定）：

Reward
 5.0 |████████████████████████████████████
     |
 4.0 |
     |
 3.0 |
     |
     +------------------------------------
     0    2k    4k    6k    8k    10k
```

---

## 🔄 历史帧缓冲

### 为什么需要历史帧？

单帧深度图只能提供**静态信息**，无法知道：
- 机器人的运动方向
- 障碍物是否在移动
- 地形的连续性

使用历史帧可以：
✅ 估计运动速度和方向
✅ 提高对动态环境的适应性
✅ 增强时序一致性

### 缓冲机制

```python
class depth:
    buffer_len = 2           # 保存最近2帧
    update_interval = 5      # 每5个仿真步(0.025s)更新一次

# 缓冲队列（FIFO）
depth_buffer = [
    frame_t,      # 当前帧 (最新)
    frame_t-1,    # 0.025秒前
]

# 每次更新时：
depth_buffer.pop(0)              # 移除最旧帧
depth_buffer.append(new_frame)   # 添加新帧

# 使用方式1: 只用最新帧
depth_input = depth_buffer[-1]   # 取最后一帧

# 使用方式2: 堆叠多帧（可选）
depth_stacked = torch.stack(depth_buffer, dim=1)  # [batch, 2, 58, 87]
# 可以用3D卷积或RNN处理时序信息
```

---

## ⚙️ 更新频率控制

### 为什么不每步都更新？

```python
控制频率: 50 Hz (每20ms一次)
仿真频率: 200 Hz (每5ms一次)
相机频率: 10 Hz (每100ms一次)  ← update_interval=5时

原因：
1. 深度相机渲染耗时（每帧~5ms）
2. 真实硬件相机帧率有限（通常10-30fps）
3. 地形变化缓慢，不需要高频更新
4. 降低计算负担，提高训练速度
```

### 更新策略

```python
# 在legged_robot.py中
def update_depth_buffer(self):
    # 每5个仿真步才更新一次
    if self.global_counter % self.cfg.depth.update_interval != 0:
        return  # 不是更新时刻，跳过
    
    # 执行图形渲染
    self.gym.render_all_camera_sensors(self.sim)
    
    # 获取并处理深度图
    for env_id in range(self.num_envs):
        depth_image = self.get_camera_image(env_id)
        depth_processed = self.process_depth_image(depth_image)
        self.depth_buffer[env_id].append(depth_processed)
```

**时间线示例：**
```
时间:   0ms   5ms  10ms  15ms  20ms  25ms  30ms  35ms  40ms ...
仿真:   ━━━┳━━━━┳━━━━┳━━━━┳━━━━┳━━━━┳━━━━┳━━━━┳━━━━┳━━━
控制:   ━━━━━━━━━━━━━━━━━━━●━━━━━━━━━━━━━━━━━━━●━━━  (50Hz)
相机:   ━━━━━━━━━━━━━━━━━━━━━━━━●━━━━━━━━━━━━━━━━━━  (10Hz)
        ↑初始    ↑skip ↑skip ↑skip ↑更新! ↑skip...
```

---

## 🚀 真机部署

### 硬件选择

推荐的深度相机：
1. **Intel RealSense D435/D455** ✅ 推荐
   - 分辨率：最高1280x720
   - 帧率：最高90fps
   - 视场角：87° (水平) × 58° (垂直) ← **完美匹配！**
   - 深度范围：0.1m - 10m
   - 价格：~$200-300

2. **Orbbec Astra**
   - 分辨率：640x480
   - 帧率：30fps
   - 视场角：60° × 49.5°
   - 价格：~$150

3. **Kinect Azure**
   - 分辨率：最高1024x1024
   - 帧率：30fps
   - 视场角：75° × 65°
   - 价格：~$400

### 部署流程

```python
# 1. 导出JIT模型
python save_jit.py --exptid 001-02

# 输出文件:
# - policy_jit.pt        (策略网络)
# - depth_encoder_jit.pt (深度编码器)

# 2. 在真机上加载
import torch

policy = torch.jit.load("policy_jit.pt")
depth_encoder = torch.jit.load("depth_encoder_jit.pt")

# 3. 实时推理循环
while True:
    # 获取深度图
    depth_image = camera.get_depth()  # [480, 640]
    
    # 预处理（匹配训练时的处理）
    depth = preprocess_depth(depth_image)  # → [58, 87]
    
    # 获取本体感知
    proprio = get_proprioception()  # 速度、姿态、关节等
    
    # 深度编码
    priv_latent = depth_encoder(depth, proprio)
    
    # 策略推理
    obs = torch.cat([proprio, scan, history, priv_latent], dim=-1)
    action = policy(obs)
    
    # 发送到电机
    robot.set_joint_targets(action)
```

### 常见问题和解决方案

#### 1. 深度图有噪声
```python
# 训练时添加噪声增强鲁棒性
class depth:
    dis_noise = 0.05  # 增加到5cm噪声

# 真机上使用滤波
depth_filtered = median_filter(depth_raw, kernel_size=3)
```

#### 2. 推理延迟大
```python
# 使用TensorRT加速
import torch_tensorrt

depth_encoder_trt = torch_tensorrt.compile(
    depth_encoder,
    inputs=[depth_input_example],
    enabled_precisions={torch.float16}  # 使用FP16
)

# 推理时间: 2ms → 0.5ms
```

#### 3. 视场角不匹配
```python
# 如果真机相机FOV=60°，训练时用87°
# 解决方案1: 使用广角镜头
# 解决方案2: 重新训练（调整config中的horizontal_fov）

class depth:
    horizontal_fov = 60  # 匹配真机相机
```

#### 4. 图像分辨率不同
```python
# 真机相机: 640x480
# 训练使用: 87x58

# 预处理时调整
def preprocess_depth(depth_raw):
    depth = cv2.resize(depth_raw, (87, 58))
    depth = normalize(depth)
    return depth
```

---

## 📈 性能指标

### 训练效率

| 项目 | 无深度相机 | 有深度相机 |
|------|-----------|-----------|
| FPS (RTX 3090) | 35,000 | 8,000 |
| 训练时间/轮 | 1秒 | 4秒 |
| 显存占用 | 8GB | 12GB |
| 总训练时间 | 8小时 | 10小时 |

### 推理性能

| 操作 | GPU | CPU |
|------|-----|-----|
| 图像预处理 | 0.1ms | 2ms |
| 深度编码器 | 2ms | 15ms |
| 策略网络 | 0.5ms | 3ms |
| **总计** | **2.6ms** | **20ms** |

**结论**：
- GPU推理：可达384 Hz (每2.6ms)
- CPU推理：可达50 Hz (每20ms)
- 真机控制频率50Hz完全够用

---

## 🎯 调优建议

### 提高视觉性能

```python
# 1. 增加训练时的深度图多样性
class depth:
    dis_noise = 0.05        # 增加距离噪声
    
class domain_rand:
    randomize_camera_angle = True
    camera_angle_range = [-10, 0]  # 随机俯仰角

# 2. 使用更大的深度编码器
depth_encoder = DepthBackbone(
    hidden_dims=[128, 256, 128]  # 增大网络
)

# 3. 增加历史帧数
class depth:
    buffer_len = 4  # 使用4帧历史

# 4. 数据增强
- 随机crop深度图
- 随机添加遮挡（模拟障碍物）
- 调整深度范围
```

### 减少计算负担

```python
# 1. 降低更新频率
class depth:
    update_interval = 10  # 从5改到10 (5Hz相机)

# 2. 降低分辨率
class depth:
    resized = (43, 29)  # 从(87,58)减半

# 3. 使用更小的网络
depth_encoder = DepthBackbone(
    hidden_dims=[64, 32]  # 更小的隐层
)

# 4. 训练时只用部分环境有相机
class depth:
    camera_num_envs = 512  # 只有512个环境渲染相机
```

---

## 📚 总结

### 视觉系统优势
✅ **直接几何感知**：深度图直接提供障碍物距离
✅ **光照鲁棒**：不受光照变化影响
✅ **训练高效**：比RGB相机训练快3-5倍
✅ **部署简单**：网络小，推理快
✅ **可解释性强**：能直接可视化机器人"看到"的内容

### 关键参数总结

```python
# 相机配置
position = [0.27, 0, 0.03]   # 前置，略向下
angle = [-5, 5]              # 俯视地面
horizontal_fov = 87          # 宽视野
resized = (87, 58)           # 网络输入尺寸

# 深度范围
near_clip = 0.0              # 0米
far_clip = 2.0               # 2米

# 更新频率
update_interval = 5          # 10Hz更新
buffer_len = 2               # 2帧历史

# 网络结构
Conv2D → MaxPool → Conv2D → FC → 32维特征
参数量: ~8M
推理时间: ~2ms (GPU)
```

### 后续优化方向
🔮 使用循环神经网络处理时序信息
🔮 多相机融合（前视+侧视）
🔮 RGB-D融合（深度+纹理）
🔮 在线自适应（域自适应）
🔮 语义分割（区分地形类型）

---

**相关文档**：
- 主教程: `TUTORIAL_CN.md`
- Tita机器人: `TITA_UPDATE_SUMMARY.md`
